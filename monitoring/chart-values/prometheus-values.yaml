defaultRules:
  create: false

additionalPrometheusRulesMap: 
  rule-name:
    groups:
    - name: GroupA
      rules:
      - alert: reviewsHTTPError
        expr: increase(istio_requests_total{source_workload=~"reviews-.*", source_app="reviews",reporter="source" ,response_code!="200"}[5m]) > 1
        for: 1m
        labels:
          severity: P1
        annotations:
          summary: reviews service error
          description: error found in reviews service
      - alert: productpageHTTPError
        expr: increase(istio_requests_total{source_workload=~"productpage-.*", source_app="productpage",reporter="source" ,response_code!="200"}[5m]) > 1
        for: 1m
        labels:
          severity: P1
        annotations:
          summary: productpage service error
          description: error found in productpage service
      - alert: ratingsHTTPError
        expr: increase(istio_requests_total{source_workload=~"ratings-.*", source_app="ratings",reporter="source" ,response_code!="200"}[5m]) > 1
        for: 1m
        labels:
          severity: P1
        annotations:
          summary: ratings service error
          description: error found in ratings service
      - alert: detailsHTTPError
        expr: increase(istio_requests_total{source_workload=~"productpage-.*", destination_app="details",reporter="source" ,response_code!="200"}[5m]) > 1
        for: 1m
        labels:
          severity: P1
        annotations:
          summary: details service error
          description: error found in details service

alertmanager:
  enabled: true
  alertmanagerSpec:
    loglevel: debug
    # configSecret: promalertmanager
  config:
    global:
      slack_api_url: "slack-webhook-url" 
      resolve_timeout: 5m
    inhibit_rules:
    route:
      group_by: ['namespace']
      group_wait: 30s
      group_interval: 2m
      repeat_interval: 2m
      receiver: 'slack'
      routes:
      - match:
          severity: P1
        receiver: 'slack'
        continue: true
    receivers:
    - name: 'null'
    - name: 'slack'
      slack_configs:
      - channel: '#sre-stack'
        send_resolved: true 
        # title: 'Monitoring Event Notification'
        # text:
    # templates:
    #   - '/etc/alertmanager/config/*.tmpl'

###kube-prometheus-stack###
grafana:
  grafana.ini:
  enabled: true
  env:
    GF_SERVER_ROOT_URL: "%(protocol)s://%(domain)s:/grafana"
    GF_SERVER_SERVE_FROM_SUB_PATH: 'true'

  sidecar:
    dashboards:
      enabled: true
      label: grafana_dashboard
      labelValue: "1"
      # Allow discovery in all namespaces for dashboards
      searchNamespace: ALL
      folder: /var/lib/grafana/dashboards
      folderAnnotation: grafana_folder
      ## Annotations for Grafana dashboard configmaps
      ##
      annotations: {}
      #annotations:
      #    grafana_folder: "Database"
      #folderAnnotation: grafana_folder
      multicluster:
        global:
          enabled: false
        etcd:
          enabled: false
      provider:
        allowUiUpdates: true 
        foldersFromFilesStructure: true
    datasources:
      enabled: true
      defaultDatasourceEnabled: true
      isDefaultDatasource: true

      uid: prometheus

      ## URL of prometheus datasource
      ##
      url: http://prometheus-stack-kube-prom-prometheus:9090

      ## Prometheus request timeout in seconds
      # timeout: 30

      # If not defined, will use prometheus.prometheusSpec.scrapeInterval or its default
      # defaultDatasourceScrapeInterval: 15s

      ## Annotations for Grafana datasource configmaps
      ##
      annotations: {}

      ## Set method for HTTP to send query to datasource
      httpMethod: POST

      ## Create datasource for each Pod of Prometheus StatefulSet;
      ## this uses headless service `prometheus-operated` which is
      ## created by Prometheus Operator
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/0fee93e12dc7c2ea1218f19ae25ec6b893460590/pkg/prometheus/statefulset.go#L255-L286
      createPrometheusReplicasDatasources: false
      label: grafana_datasource
      labelValue: "1"

      ## Field with internal link pointing to existing data source in Grafana.
      ## Can be provisioned via additionalDataSources
      exemplarTraceIdDestinations: {}
        # datasourceUid: Jaeger
        # traceIdLabelName: trace_id
      alertmanager:
        enabled: true
        uid: alertmanager
        handleGrafanaManagedAlerts: false
        implementation: prometheus

  extraConfigmapMounts: 


  deleteDatasources: []
  # - name: example-datasource
  #   orgId: 1

  ## Configure additional grafana datasources (passed through tpl)
  ## ref: http://docs.grafana.org/administration/provisioning/#datasources
  additionalDataSources: 
  - name: loki
    access: proxy
    orgId: 1
    type: loki
    url: http://loki:3100
    version: 1
  # - name: prometheus-sample
  #   access: proxy
  #   basicAuth: true
  #   basicAuthPassword: pass
  #   basicAuthUser: daco
  #   editable: false
  #   jsonData:
  #       tlsSkipVerify: true
  #   orgId: 1
  #   type: prometheus
  #   url: https://{{ printf "%s-prometheus.svc" .Release.Name }}:9090
  #   version: 1



  # # readinessProbe:
  # # httpGet:
  # #   path: /api/health
  # #   port: 80

  # # livenessProbe:
  # #   httpGet:
  # #     path: /api/health
  # #     port: 80
  # #   initialDelaySeconds: 60
  # #   timeoutSeconds: 30
  # #   failureThreshold: 10
  # datasources: 
  #   datasources.yaml:
  #     apiVersion: 1
  #     datasources:
  #     - name: Prometheus
  #       type: prometheus
  #       url: http://prometheus-stack-kube-prom-prometheus:9090
  #       isDefault: true
  #       enabled: true
  #       defaultDatasourceEnabled: true
  #       isDefaultDatasource: true

  # dashboardProviders: 
  #   dashboardproviders.yaml:
  #     apiVersion: 1
  #     providers:
  #     - name: 'default'
  #       orgId: 1
  #       folder: 'dashboards'
  #       type: file
  #       disableDeletion: false
  #       editable: true
  #       options:
  #         path: /var/lib/grafana/dashboards/default

  # dashboards:
  #   default:
  #     loki-dashboard-quick-search:
  #       gnetId: 7645
  #       revision: 165
  #       datasource:
  #       - name: DS_PROMETHEUS
  #         value: Prometheus
  #     kubernetes-cluster-monitoring:
  #       gnetId: 6417
  #       revision: 1
  #       datasource:
  #       - name: DS_PROMETHEUS
  #         value: Prometheus
  #     node-exporter-monitoring:
  #       gnetId: 1860
  #       revision: 31
  #       datasource:
  #       - name: DS_PROMETHEUS
  #         value: Prometheus
  #     loki-monitoring:
  #       gnetId: 14055
  #       revision: 5
  #       datasource:
  #       - name: DS_PROMETHEUS
  #         value: Prometheus
  #       # - name: DS_PROMETHEUS
  #       #   value: Loki
  #     k8s-views-pods-monitoring:
  #       gnetId: 15760
  #       revision: 18
  #       datasource:
  #       - name: DS_PROMETHEUS
  #         value: Prometheus
  #     k8s-cluster-monitoring:
  #       gnetId: 1621
  #       revision: 1
  #       datasource:
  #       - name: DS_PROMETHEUS
  #         value: Prometheus
  #     k8s-views-namespace-monitoring:
  #       gnetId: 15758
  #       revision: 23
  #       datasource:
  #       - name: DS_PROMETHEUS
  #         value: Prometheus
  #     k8s-views-nodes-monitoring:
  #       gnetId: 15759
  #       revision: 19
  #       datasource:
  #       - name: DS_PROMETHEUS
  #         value: Prometheus
  #     k8s-views-api-server-monitoring:
  #       gnetId: 15761
  #       revision: 12
  #       datasource:
  #       - name: DS_PROMETHEUS
  #         value: Prometheus
  #     k8s-views-core-dns-monitoring:
  #       gnetId: 15762
  #       revision: 12
  #       datasource:
  #       - name: DS_PROMETHEUS
  #         value: Prometheus

  service:
    enabled: true
    # type: LoadBalancer
    port: 80
    targetPort: 3000
prometheus:
  enabled: true
  service:
      ## Port for Prometheus Service to listen on
      ##
      port: 9090
      ## Service type
      # type: LoadBalancer
  prometheusSpec:
    enableRemoteWriteReceiver: true
    externalUrl: "https://localhost:9090/prometheus/"
    additionalScrapeConfigs:
      - job_name: 'istiod'
        kubernetes_sd_configs:
        - role: endpoints
          namespaces:
            names:
            - istio-system
        relabel_configs:
        - source_labels: [__meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
          action: keep
          regex: istiod;http-monitoring
      - job_name: 'envoy-stats'
        metrics_path: /stats/prometheus
        kubernetes_sd_configs:
        - role: pod
        relabel_configs:
        - source_labels: [__meta_kubernetes_pod_container_port_name]
          action: keep
          regex: '.*-envoy-prom'
  additionalServiceMonitors:
      - name: prometheus-operator-cloudwatch
        endpoints:
        - port: http
          path: /metrics
          interval: 30s
        namespaceSelector:
          matchNames:
          - monitoring
        selector:
          matchLabels:
            app.kubernetes.io/name: yet-another-cloudwatch-exporter